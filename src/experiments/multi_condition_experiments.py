import pandas as pd
import numpy as np
import logging
import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from core.data_loader import DataLoader
from utils.config import get_db_connection_string
from evaluation.evaluator import Evaluator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultiConditionExperiment:
    """å¤šæŸ¥è¯¢æ¡ä»¶å®éªŒ - å®Œæ•´ç‰ˆæœ¬"""
    
    def __init__(self):
        self.loader = DataLoader(get_db_connection_string())
    
    def run_experiment_for_condition(self, market_segment, date='1995-03-15'):
        """ä¸ºç‰¹å®šå¸‚åœºç»†åˆ†å’Œæ—¥æœŸè¿è¡Œå®éªŒ"""
        condition_name = f"{market_segment}_{date.replace('-', '')}"
        
        print(f"\n{'='*60}")
        print(f"è¿è¡Œå®éªŒ: {condition_name}")
        print(f"{'='*60}")
        
        try:
            # ä½¿ç”¨æŒ‡å®šçš„å¸‚åœºç»†åˆ†å’Œæ—¥æœŸè·å–æ•°æ®
            ground_truth = self.loader.get_ground_truth(market_segment, date)
            contributions = self.loader.get_customer_contributions(market_segment, date)
            
            if ground_truth.empty or len(ground_truth) < 5:  # è‡³å°‘éœ€è¦5ä¸ªç»“æœæ‰æœ‰æ„ä¹‰
                print(f"  âš ï¸ {condition_name} æ•°æ®ä¸è¶³({len(ground_truth)}æ¡)ï¼Œè·³è¿‡")
                return pd.DataFrame()
            
            print(f"æ•°æ®ç‰¹å¾:")
            print(f"  â€¢ å®¢æˆ·æ•°é‡: {contributions['c_custkey'].nunique()}")
            print(f"  â€¢ è®¢å•é¡¹æ•°é‡: {len(contributions)}")
            print(f"  â€¢ Top-10è®¢å•æ•°é‡: {len(ground_truth)}")
            print(f"  â€¢ Top-10è®¢å•æ€»æ”¶å…¥: {ground_truth['revenue'].sum():.2f}")
            
            # åˆ†ææ•æ„Ÿåº¦
            customer_totals = contributions.groupby('c_custkey')['contribution'].sum()
            delta_f = customer_totals.max()
            sensitivity_ratio = delta_f / ground_truth['revenue'].mean() if ground_truth['revenue'].mean() > 0 else 0
            
            print(f"  â€¢ å…¨å±€æ•æ„Ÿåº¦ Î”f: {delta_f:.2f}")
            print(f"  â€¢ Î”f/å¹³å‡æ”¶å…¥: {sensitivity_ratio:.2f}")
            
            results = []
            
            # æµ‹è¯•å„ç§æœºåˆ¶
            mechanisms = [
                ('NaiveLaplace', self._run_naive),
                ('Adaptive_R2T', self._run_adaptive_r2t),
                ('Adaptive_SI', self._run_adaptive_si)
            ]
            
            for method_name, mechanism_func in mechanisms:
                try:
                    # ä¼ é€’å¸‚åœºç»†åˆ†å’Œæ—¥æœŸå‚æ•°
                    result = mechanism_func(market_segment, date, epsilon=1.0, random_state=42)
                    if result is None or result.empty:
                        continue
                        
                    evaluator = Evaluator(ground_truth)
                    metrics = evaluator.evaluate_all(result, method_name)
                    
                    # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
                    metrics['method'] = method_name
                    metrics['condition'] = condition_name
                    metrics['market_segment'] = market_segment
                    metrics['date'] = date
                    metrics['delta_f'] = delta_f
                    metrics['sensitivity_ratio'] = sensitivity_ratio
                    metrics['customer_count'] = contributions['c_custkey'].nunique()
                    metrics['item_count'] = len(contributions)
                    metrics['topk_count'] = len(ground_truth)
                    
                    results.append(metrics)
                    
                    print(f"  {method_name}: è¯¯å·®={metrics['relative_error']:.3f}, Tau={metrics['kendall_tau']:.3f}")
                    
                except Exception as e:
                    print(f"  {method_name} å¤±è´¥: {e}")
                    continue
            
            return pd.DataFrame(results)
            
        except Exception as e:
            print(f"  {condition_name} æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _run_naive(self, market_segment, date, epsilon, random_state):
        """è¿è¡Œæœ´ç´ æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶"""
        from core.naive_laplace import NaiveLaplaceMechanism
        mechanism = NaiveLaplaceMechanism(self.loader)
        return mechanism.run_mechanism(
            epsilon=epsilon, 
            market_segment=market_segment,
            date=date,
            random_state=random_state
        )
    
    def _run_adaptive_r2t(self, market_segment, date, epsilon, random_state):
        """è¿è¡Œè‡ªé€‚åº”R2T"""
        from core.r2t import R2TMechanism
        
        try:
            # ä½¿ç”¨æŒ‡å®šå‚æ•°è·å–è´¡çŒ®æ•°æ®
            contributions = self.loader.get_customer_contributions(market_segment, date)
            
            # åŸºäºåˆ†ä½æ•°é€‰æ‹©Tå€¼
            customer_stats = contributions.groupby('c_custkey').agg({
                'contribution': ['sum', 'count']
            })
            item_counts = customer_stats[('contribution', 'count')]
            
            quantiles = item_counts.quantile([0.25, 0.5, 0.75])
            T_candidates = [max(1, int(q)) for q in quantiles]
            
            mechanism = R2TMechanism(self.loader)
            return mechanism.run_mechanism(
                epsilon=epsilon, 
                T_list=T_candidates,
                market_segment=market_segment,
                date=date,
                random_state=random_state
            )
        except Exception as e:
            print(f"  R2Tæœºåˆ¶å¤±è´¥: {e}")
            return None
    
    def _run_adaptive_si(self, market_segment, date, epsilon, random_state):
        """è¿è¡Œè‡ªé€‚åº”Shifted Inverse"""
        from core.shifted_inverse import ShiftedInverseMechanism
        
        try:
            # ä½¿ç”¨æŒ‡å®šå‚æ•°è·å–è´¡çŒ®æ•°æ®
            contributions = self.loader.get_customer_contributions(market_segment, date)
            
            # åŸºäºåˆ†ä½æ•°é€‰æ‹©Tå€¼
            customer_stats = contributions.groupby('c_custkey').agg({
                'contribution': ['sum', 'count']
            })
            customer_totals = customer_stats[('contribution', 'sum')]
            
            quantiles = customer_totals.quantile([0.25, 0.5, 0.75])
            T_candidates = [max(1000, int(q)) for q in quantiles]
            
            mechanism = ShiftedInverseMechanism(self.loader)
            return mechanism.run_mechanism(
                epsilon=epsilon, 
                T_list=T_candidates,
                market_segment=market_segment,
                date=date,
                random_state=random_state
            )
        except Exception as e:
            print(f"  Shifted Inverseæœºåˆ¶å¤±è´¥: {e}")
            return None
    
    def run_complete_experiment(self):
        """è¿è¡Œå®Œæ•´çš„å¤šæ¡ä»¶å®éªŒ"""
        all_results = []
        
        # å®Œæ•´çš„æµ‹è¯•æ¡ä»¶ï¼š5ä¸ªå¸‚åœºç»†åˆ† Ã— 5ä¸ªæ—¥æœŸ
        market_segments = ['BUILDING', 'AUTOMOBILE', 'MACHINERY', 'HOUSEHOLD', 'FURNITURE']
        dates = ['1995-03-01', '1995-03-08', '1995-03-15', '1995-03-22', '1995-03-29']
        
        print("å¼€å§‹å®Œæ•´å¤šæ¡ä»¶å®éªŒ...")
        print(f"æµ‹è¯•å¸‚åœºç»†åˆ†: {market_segments}")
        print(f"æµ‹è¯•æ—¥æœŸ: {dates}")
        print(f"æ€»æµ‹è¯•æ¡ä»¶: {len(market_segments) * len(dates)}")
        print(f"{'='*60}")
        
        total_conditions = len(market_segments) * len(dates)
        completed_conditions = 0
        
        for segment in market_segments:
            for date in dates:
                try:
                    print(f"\nè¿›åº¦: {completed_conditions + 1}/{total_conditions}")
                    condition_results = self.run_experiment_for_condition(segment, date)
                    if not condition_results.empty:
                        all_results.append(condition_results)
                        completed_conditions += 1
                        print(f"âœ… å®Œæˆ {segment} {date} çš„å®éªŒ")
                    else:
                        print(f"âš ï¸ {segment} {date} æ— æœ‰æ•ˆç»“æœ")
                except Exception as e:
                    print(f"âŒ {segment} {date} å®éªŒå¤±è´¥: {e}")
                    continue
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        if all_results:
            final_results = pd.concat(all_results, ignore_index=True)
            print(f"\n{'='*60}")
            print("å®éªŒå®Œæˆæ€»ç»“")
            print(f"{'='*60}")
            print(f"æ€»æµ‹è¯•æ¡ä»¶: {total_conditions}")
            print(f"æˆåŠŸå®Œæˆ: {completed_conditions}")
            print(f"æˆåŠŸç‡: {completed_conditions/total_conditions*100:.1f}%")
            
            self.analyze_comprehensive_results(final_results)
            return final_results
        else:
            print("æ‰€æœ‰å®éªŒå‡æ— æœ‰æ•ˆç»“æœ")
            return pd.DataFrame()
    
    def analyze_comprehensive_results(self, results):
        """åˆ†æå®Œæ•´å®éªŒç»“æœ"""
        print(f"\n{'='*60}")
        print("å®Œæ•´å¤šæ¡ä»¶å®éªŒç»“æœåˆ†æ")
        print(f"{'='*60}")
        
        if results.empty:
            print("æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return
        
        print(f"æ€»è®°å½•æ•°: {len(results)}")
        print(f"æˆåŠŸæµ‹è¯•æ¡ä»¶æ•°: {results['condition'].nunique()}")
        print(f"æ¶‰åŠå¸‚åœºç»†åˆ†: {sorted(results['market_segment'].unique())}")
        print(f"æ¶‰åŠæ—¥æœŸèŒƒå›´: {sorted(results['date'].unique())}")
        print(f"æ–¹æ³•åˆ†å¸ƒ: {results['method'].value_counts().to_dict()}")
        
        # è¯¦ç»†æ€§èƒ½åˆ†æ
        print(f"\n{'='*40}")
        print("è¯¦ç»†æ€§èƒ½åˆ†æ")
        print(f"{'='*40}")
        
        # æŒ‰æ–¹æ³•åˆ†ç»„çš„æ€»ä½“æ€§èƒ½
        overall_performance = results.groupby('method').agg({
            'relative_error': ['mean', 'std', 'min', 'max'],
            'kendall_tau': ['mean', 'std', 'min', 'max'],
            'jaccard': 'mean'
        }).round(4)
        
        print("å„æ–¹æ³•æ€»ä½“æ€§èƒ½:")
        print(overall_performance)
        
        # æŒ‰å¸‚åœºç»†åˆ†åˆ†æ
        segment_analysis = results.groupby(['market_segment', 'method']).agg({
            'relative_error': 'mean',
            'kendall_tau': 'mean',
            'delta_f': 'first',
            'sensitivity_ratio': 'first',
            'customer_count': 'first'
        }).round(4)
        
        print(f"\nå„å¸‚åœºç»†åˆ†è¯¦ç»†æ€§èƒ½:")
        print(segment_analysis)
        
        # æ•æ„Ÿåº¦ä¸æ€§èƒ½ç›¸å…³æ€§åˆ†æ
        print(f"\n{'='*40}")
        print("æ•æ„Ÿåº¦ä¸æ€§èƒ½ç›¸å…³æ€§åˆ†æ")
        print(f"{'='*40}")
        
        correlation_data = results[['sensitivity_ratio', 'relative_error', 'kendall_tau', 'customer_count']].corr()
        print("ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ:")
        print(correlation_data.round(3))
        
        # æ€§èƒ½ç¨³å®šæ€§åˆ†æ
        stability_analysis = results.groupby('method').agg({
            'relative_error': 'std',
            'kendall_tau': 'std'
        }).round(4)
        
        print(f"\næ–¹æ³•æ€§èƒ½ç¨³å®šæ€§(æ ‡å‡†å·®):")
        print(stability_analysis)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_dir = '../results'
        os.makedirs(output_dir, exist_ok=True)
        
        results.to_csv(f'{output_dir}/comprehensive_results.csv', index=False)
        overall_performance.to_csv(f'{output_dir}/overall_performance.csv')
        segment_analysis.to_csv(f'{output_dir}/segment_analysis.csv')
        
        # ç”Ÿæˆç»¼åˆå¯è§†åŒ–
        self._create_comprehensive_visualizations(results)
        
        print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {output_dir}/ ç›®å½•")
    
    def _create_comprehensive_visualizations(self, results):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨"""
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        if results.empty:
            print("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå¯è§†åŒ–")
            return
        
        # åˆ›å»º2x3çš„å­å›¾å¸ƒå±€
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Comprehensive Multi-Condition Analysis', fontsize=16, fontweight='bold')
        
        # 1. å„æ–¹æ³•æ€§èƒ½å¯¹æ¯”çƒ­åŠ›å›¾
        if all(col in results.columns for col in ['market_segment', 'method', 'relative_error']):
            performance_pivot = results.pivot_table(
                values='relative_error', 
                index='market_segment', 
                columns='method', 
                aggfunc='mean'
            )
            sns.heatmap(performance_pivot, annot=True, cmap='YlOrRd_r', 
                       cbar_kws={'label': 'Relative Error'}, ax=axes[0,0])
            axes[0,0].set_title('A. Relative Error by Segment and Method')
        
        # 2. æ’åºè´¨é‡çƒ­åŠ›å›¾
        if all(col in results.columns for col in ['market_segment', 'method', 'kendall_tau']):
            tau_pivot = results.pivot_table(
                values='kendall_tau', 
                index='market_segment', 
                columns='method', 
                aggfunc='mean'
            )
            sns.heatmap(tau_pivot, annot=True, cmap='RdYlBu', center=0,
                       cbar_kws={'label': 'Kendall Tau'}, ax=axes[0,1])
            axes[0,1].set_title('B. Ranking Quality by Segment and Method')
        
        # 3. æ•æ„Ÿåº¦ä¸è¯¯å·®å…³ç³»
        if all(col in results.columns for col in ['sensitivity_ratio', 'relative_error', 'method']):
            for method in results['method'].unique():
                method_data = results[results['method'] == method]
                axes[0,2].scatter(method_data['sensitivity_ratio'], 
                                method_data['relative_error'], 
                                label=method, alpha=0.6, s=60)
            axes[0,2].set_xlabel('Sensitivity Ratio (Î”f/avg_revenue)')
            axes[0,2].set_ylabel('Relative Error')
            axes[0,2].set_title('C. Sensitivity vs Revenue Error')
            axes[0,2].legend()
            axes[0,2].grid(True, alpha=0.3)
        
        # 4. å®¢æˆ·æ•°é‡ä¸æ€§èƒ½å…³ç³»
        if all(col in results.columns for col in ['customer_count', 'relative_error', 'method']):
            for method in results['method'].unique():
                method_data = results[results['method'] == method]
                axes[1,0].scatter(method_data['customer_count'], 
                                method_data['relative_error'], 
                                label=method, alpha=0.6, s=60)
            axes[1,0].set_xlabel('Customer Count')
            axes[1,0].set_ylabel('Relative Error')
            axes[1,0].set_title('D. Customer Count vs Performance')
            axes[1,0].legend()
            axes[1,0].grid(True, alpha=0.3)
        
        # 5. æ–¹æ³•æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾
        if 'method' in results.columns and 'relative_error' in results.columns:
            sns.boxplot(data=results, x='method', y='relative_error', ax=axes[1,1])
            axes[1,1].set_title('E. Revenue Error Distribution by Method')
            axes[1,1].tick_params(axis='x', rotation=45)
        
        # 6. æ’åºè´¨é‡åˆ†å¸ƒç®±çº¿å›¾
        if 'method' in results.columns and 'kendall_tau' in results.columns:
            sns.boxplot(data=results, x='method', y='kendall_tau', ax=axes[1,2])
            axes[1,2].set_title('F. Ranking Quality Distribution by Method')
            axes[1,2].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('../results/comprehensive_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ç»¼åˆå¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: ../results/comprehensive_analysis.png")

def main():
    """ä¸»å‡½æ•°"""
    experiment = MultiConditionExperiment()
    results = experiment.run_complete_experiment()
    
    if not results.empty:
        print(f"\nğŸ‰ å®Œæ•´å®éªŒå®Œæˆ!")
        print(f"å…±æˆåŠŸæµ‹è¯• {results['condition'].nunique()} ä¸ªæ¡ä»¶")
        print(f"æ”¶é›† {len(results)} æ¡ç»“æœè®°å½•")
        print(f"æ¶‰åŠæ‰€æœ‰5ä¸ªç»†åˆ†å¸‚åœº")
        print(f"è¯¦ç»†ç»“æœå’Œå›¾è¡¨å·²ä¿å­˜åˆ° ../results/ ç›®å½•")
    else:
        print("\nå®éªŒå®Œæˆï¼Œä½†æ²¡æœ‰è·å¾—æœ‰æ•ˆç»“æœ")

if __name__ == "__main__":
    main()